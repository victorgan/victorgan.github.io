---
layout: post
title: Machine Learning Notes
---

## Notes from news in machine learning.

### October 2014

Deep Learning: Methods and Applications 

- [2014-09-19-deeplearning]({{ site.url }}/assets/posts/2014-09-19-deeplearning.pdf)

### September 2014
(http://image-net.org/challenges/LSVRC/2014/slides/GoogLeNet.pptx)

[Microsoft ML Hackathon](http://blogs.technet.com/b/machinelearning/archive/2014/09/16/microsoft-machine-learning-hackathon-2014.aspx): 

> it was important to be working in teams and to be thoughtful about how to
> split a given ML challenge between team members.  Assigning roles and having
> clarity on what tools to use were critical considerations.

> We have seen similar patterns in other cases too, and boosted trees are a
> strong candidate on many ML tasks. If you have some special or temporal
> structure to the data, it may be easier to encode it using neural-nets
> (although exploiting it may be non-trivial). However, if there is no structure
> to the features or if you have a limited amount of time to spend on a problem,
> one should definitely consider boosted trees.

> In fact one of our Top 5 teams comprised entirely of summer interns who were
> new to this space.

## Questions
Questions I have, or once had.

#### What's the difference between boosted trees and random forests?

They both combine many decision trees.
Boosted trees uses boosting, random forest uses bagging.

#### What are common classifiers?

#### Any online plug-n-play ML solutions?
